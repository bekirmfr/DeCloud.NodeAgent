#cloud-config

# DeCloud Inference VM Configuration
# For AI model serving and inference workloads

hostname: __HOSTNAME__
manage_etc_hosts: true

# Regenerate machine-id to prevent conflicts
bootcmd:
  - rm -f /etc/machine-id /var/lib/dbus/machine-id
  - systemd-machine-id-setup

# Inference nodes use dedicated inference user
disable_root: true

users:
  - name: inference
    groups: [docker, sudo, video, render]
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    shell: /bin/bash
    lock_passwd: true

# SSH key authentication for inference user
__SSH_AUTHORIZED_KEYS__

ssh_pwauth: false

# Inference-specific packages
packages:
  - qemu-guest-agent
  - docker.io
  - docker-compose
  - nvidia-container-toolkit
  - python3
  - python3-pip
  - git
  - htop
  - nvtop

# Inference configuration files
write_files:
  # Inference node configuration
  - path: /etc/decloud-inference/config.json
    permissions: '0644'
    content: |
      {
        "node_id": "__VM_ID__",
        "node_name": "__VM_NAME__",
        "inference_port": __INFERENCE_PORT__,
        "model_cache_path": "__MODEL_CACHE_PATH__",
        "max_batch_size": __INFERENCE_MAX_BATCH_SIZE__,
        "max_concurrent_requests": __INFERENCE_MAX_CONCURRENT__,
        "gpu_enabled": __INFERENCE_GPU_ENABLED__,
        "supported_models": [
          "llama-2-7b",
          "llama-2-13b",
          "mistral-7b",
          "stable-diffusion-xl",
          "whisper-large-v3"
        ],
        "enable_quantization": true,
        "quantization_bits": 4
      }

  # Inference API systemd service
  - path: /etc/systemd/system/decloud-inference.service
    permissions: '0644'
    content: |
      [Unit]
      Description=DeCloud Inference API
      After=network.target docker.service
      Requires=docker.service
      
      [Service]
      Type=simple
      User=inference
      WorkingDirectory=/opt/decloud-inference
      ExecStart=/usr/bin/docker-compose up
      ExecStop=/usr/bin/docker-compose down
      Restart=always
      RestartSec=10
      
      [Install]
      WantedBy=multi-user.target

  # Docker Compose for Inference API
  - path: /opt/decloud-inference/docker-compose.yml
    permissions: '0644'
    owner: inference:inference
    content: |
      version: '3.8'
      
      services:
        inference-api:
          image: decloud/inference-api:latest
          container_name: inference-__VM_ID__
          restart: unless-stopped
          ports:
            - "__INFERENCE_PORT__:8000"
          volumes:
            - model-cache:__MODEL_CACHE_PATH__
            - /etc/decloud-inference/config.json:/config/config.json:ro
          environment:
            - NODE_ID=__VM_ID__
            - LOG_LEVEL=info
            - CUDA_VISIBLE_DEVICES=all
          deploy:
            resources:
              reservations:
                devices:
                  - driver: nvidia
                    count: all
                    capabilities: [gpu]
          networks:
            - inference-network
      
      volumes:
        model-cache:
          driver: local
      
      networks:
        inference-network:
          driver: bridge

  # Nginx reverse proxy for inference API
  - path: /etc/nginx/sites-available/inference-proxy
    permissions: '0644'
    content: |
      upstream inference_backend {
          server localhost:8000;
      }
      
      server {
          listen 80 default_server;
          listen [::]:80 default_server;
          
          server_name _;
          
          client_max_body_size 100M;
          
          # Health check
          location /health {
              access_log off;
              proxy_pass http://inference_backend;
          }
          
          # Inference endpoint
          location /v1/ {
              proxy_pass http://inference_backend;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              
              # Long timeout for inference
              proxy_read_timeout 300s;
              proxy_connect_timeout 10s;
          }
          
          # WebSocket for streaming responses
          location /v1/stream {
              proxy_pass http://inference_backend;
              proxy_http_version 1.1;
              proxy_set_header Upgrade $http_upgrade;
              proxy_set_header Connection "upgrade";
          }
      }

# Runtime commands
runcmd:
  # Start QEMU guest agent
  - systemctl enable qemu-guest-agent
  - systemctl start qemu-guest-agent
  
  # Create inference directories
  - mkdir -p /etc/decloud-inference
  - mkdir -p __MODEL_CACHE_PATH__
  - mkdir -p /opt/decloud-inference
  - chown -R inference:inference /etc/decloud-inference
  - chown -R inference:inference __MODEL_CACHE_PATH__
  - chown -R inference:inference /opt/decloud-inference
  
  # Enable Docker
  - systemctl enable docker
  - systemctl start docker
  - usermod -aG docker inference
  
  # Configure NVIDIA Container Toolkit (if GPU present)
  - nvidia-ctk runtime configure --runtime=docker || true
  - systemctl restart docker || true
  
  # Pull inference image
  - docker pull decloud/inference-api:latest || true
  
  # Configure Nginx
  - apt-get install -y nginx
  - ln -sf /etc/nginx/sites-available/inference-proxy /etc/nginx/sites-enabled/
  - rm -f /etc/nginx/sites-enabled/default
  - systemctl enable nginx
  - systemctl restart nginx
  
  # Start inference service
  - systemctl daemon-reload
  - systemctl enable decloud-inference
  - systemctl start decloud-inference
  
  # Log completion
  - echo "DeCloud Inference node __VM_ID__ initialized" > /var/log/inference-bootstrap.log

final_message: "DeCloud Inference Node __VM_NAME__ is ready!"
